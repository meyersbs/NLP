{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Basic Topic Modeling with Gensim"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Turn off annoying warning messages\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import matplotlib.pyplot as PLT\n",
    "import numpy as NP\n",
    "import pandas as PD\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "# Turn off SpaCy's parser and named-entity-recognition since we only need its POS tagger\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rec.autos' 'comp.sys.mac.hardware' 'rec.motorcycles' 'misc.forsale'\n 'comp.os.ms-windows.misc' 'alt.atheism' 'comp.graphics'\n 'rec.sport.baseball' 'rec.sport.hockey' 'sci.electronics' 'sci.space'\n 'talk.politics.misc' 'sci.med' 'talk.politics.mideast'\n 'soc.religion.christian' 'comp.windows.x' 'comp.sys.ibm.pc.hardware'\n 'talk.politics.guns' 'talk.religion.misc' 'sci.crypt']\n"
     ]
    }
   ],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "df = PD.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
    "print(df.target_names.unique())\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['From: (wheres my thing) Subject: WHAT car is this!? Nntp-Posting-Host: '\n 'rac3.wam.umd.edu Organization: University of Maryland, College Park Lines: '\n '15 I was wondering if anyone out there could enlighten me on this car I saw '\n 'the other day. It was a 2-door sports car, looked to be from the late 60s/ '\n 'early 70s. It was called a Bricklin. The doors were really small. In '\n 'addition, the front bumper was separate from the rest of the body. This is '\n 'all I know. If anyone can tellme a model name, engine specs, years of '\n 'production, where this car is made, history, or whatever info you have on '\n 'this funky looking car, please e-mail. Thanks, - IL ---- brought to you by '\n 'your neighborhood Lerxst ---- ']\n"
     ]
    }
   ],
   "source": [
    "# Convert to list\n",
    "data = df.content.values.tolist()\n",
    "# Remove email addresses\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "# Remove '\\n' and '\\r'\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "# Remove single quotes\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "pprint(data[:1])\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['from', 'wheres', 'my', 'thing', 'subject', 'what', 'car', 'is', 'this', 'nntp', 'posting', 'host', 'rac', 'wam', 'umd', 'edu', 'organization', 'university', 'of', 'maryland', 'college', 'park', 'lines', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'saw', 'the', 'other', 'day', 'it', 'was', 'door', 'sports', 'car', 'looked', 'to', 'be', 'from', 'the', 'late', 'early', 'it', 'was', 'called', 'bricklin', 'the', 'doors', 'were', 'really', 'small', 'in', 'addition', 'the', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body', 'this', 'is', 'all', 'know', 'if', 'anyone', 'can', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'of', 'production', 'where', 'this', 'car', 'is', 'made', 'history', 'or', 'whatever', 'info', 'you', 'have', 'on', 'this', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def sentToTokens(sents):\n",
    "    for sent in sents:\n",
    "        # `deacc=True` strips punctuation\n",
    "        yield(gensim.utils.simple_preprocess(str(sent), deacc=True))\n",
    "\n",
    "\n",
    "data_tokens = list(sentToTokens(data))\n",
    "print(data_tokens[:1])\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(texts):\n",
    "    return [\n",
    "        [w for w in simple_preprocess(str(doc)) if w not in stop_words]\n",
    "        for doc in texts\n",
    "    ]\n",
    "\n",
    "\n",
    "data_tokens_clean = removeStopwords(data_tokens)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "N-Gram Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['wheres', 'thing', 'car', 'nntp_posting', 'host', 'rac_wam', 'umd', 'organization', 'university', 'maryland_college', 'park', 'lines', 'wondering', 'anyone', 'could', 'enlighten', 'car', 'saw', 'day', 'door', 'sports', 'car', 'looked', 'late', 'early', 'called', 'bricklin', 'doors', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'years', 'production', 'car', 'made', 'history', 'whatever', 'info', 'funky', 'looking', 'car', 'please', 'mail', 'thanks', 'il', 'brought', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "# A higher threshold results in fewer phrases\n",
    "bigrammer_init = gensim.models.Phrases(data_tokens, min_count=5, threshold=100)\n",
    "trigrammer_init = gensim.models.Phrases(bigrammer_init[data_tokens], threshold=100)\n",
    "\n",
    "bigrammer = gensim.models.phrases.Phraser(bigrammer_init)\n",
    "trigrammer = gensim.models.phrases.Phraser(trigrammer_init)\n",
    "\n",
    "def makeBigrams(texts):\n",
    "    return [bigrammer[doc] for doc in texts]\n",
    "\n",
    "\n",
    "def makeTrigrams(texts):\n",
    "    return [trigrammer[bigrammer[doc]] for doc in texts]\n",
    "\n",
    "\n",
    "data_bigrams = makeBigrams(data_tokens_clean)\n",
    "print(data_bigrams[:1])\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "More Data Cleaning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['where', 's', 'thing', 'car', 'nntp_poste', 'host', 'umd', 'organization', 'university', 'maryland_college', 'park', 'line', 'wonder', 'anyone', 'could', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'front_bumper', 'separate', 'rest', 'body', 'know', 'anyone', 'tellme', 'model', 'name', 'engine', 'specs', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']]\n"
     ]
    }
   ],
   "source": [
    "def lemmatize(texts, allowed_pos=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = list()\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append([tok.lemma_ for tok in doc if tok.pos_ in allowed_pos])\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "data_lemmas = lemmatize(data_bigrams)\n",
    "print(data_lemmas[:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
